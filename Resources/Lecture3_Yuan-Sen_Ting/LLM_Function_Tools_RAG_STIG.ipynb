{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56904f45",
   "metadata": {},
   "source": [
    "# Large Language Models as Research Agents: Part 2 - Function Tools and RAG\n",
    "\n",
    "*NASA Cosmic Origins AI/ML STIG Tutorial Series*\n",
    "\n",
    "*Yuan-Sen Ting (OSU)*\n",
    "\n",
    "*Part 2 of 2: Advanced LLM Capabilities*\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand and implement function tools to extend LLM capabilities\n",
    "- Build astronomical calculation tools that Claude can use automatically\n",
    "- Master Retrieval Augmented Generation (RAG) for document-based Q&A\n",
    "- Implement document chunking and embedding-based search\n",
    "- Combine function tools with RAG for powerful research assistants\n",
    "- Understand vector databases for production RAG systems\n",
    "\n",
    "---\n",
    "\n",
    "**Attribution:**  \n",
    "This material is adapted from [*Coding Essentials for Astronomers*](https://tingyuansen.github.io/coding_essential_for_astronomers/) by Yuan-Sen Ting.\n",
    "\n",
    "**Citation:**  \n",
    "Ting, Y.-S. (2025). *Coding Essentials for Astronomers*. Zenodo. [DOI: 10.5281/zenodo.17850426](https://doi.org/10.5281/zenodo.17850426)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d314f7f2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Part 1 of this series, you learned the fundamentals of working with LLM APIs—making calls, \n",
    "managing conversations, and crafting effective prompts. While powerful, those interactions were \n",
    "limited to what the LLM knows from its training data.\n",
    "\n",
    "In this lecture, we'll break through those limitations by exploring two advanced capabilities:\n",
    "\n",
    "### 1. Function Tools (Tool Use)\n",
    "Function tools allow Claude to use external functions you define. Instead of just generating text, \n",
    "Claude can:\n",
    "- Perform precise calculations (compute luminosity distance, redshift corrections)\n",
    "- Access real-time data (query databases, fetch current observations)\n",
    "- Execute complex workflows (run simulations, process data pipelines)\n",
    "\n",
    "### 2. Retrieval Augmented Generation (RAG)\n",
    "RAG enables Claude to work with your documents and data by:\n",
    "- Searching through large document collections (papers, observing logs, documentation)\n",
    "- Answering questions based on your specific data, not just general knowledge\n",
    "- Keeping responses grounded in authoritative sources\n",
    "\n",
    "**Prerequisites:** This lecture assumes you're comfortable with the API basics from Part 1, \n",
    "including making API calls, managing conversations, and understanding response structures. \n",
    "If you need a refresher, please review Part 1 first.\n",
    "\n",
    "By the end of this tutorial, you'll be able to build sophisticated research assistants that can \n",
    "both reason over your documents and perform calculations—essential tools for modern astronomical research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669e30d",
   "metadata": {},
   "source": [
    "### Why Function Tools Transform Your Research\n",
    "\n",
    "In Part 1, Claude could only respond with text. If you asked \"What's the luminosity distance to z=0.5?\", \n",
    "Claude might give an approximate answer, but it couldn't perform the exact calculation.\n",
    "\n",
    "**Function tools change this fundamentally.** They allow Claude to:\n",
    "\n",
    "1. **Recognize when a calculation is needed** (\"I need the luminosity distance\")\n",
    "2. **Request the right function** (\"Use the `luminosity_distance` function\")\n",
    "3. **Provide the correct parameters** (`redshift=0.5, H0=70, ...`)\n",
    "4. **Integrate the result into its response** (\"The luminosity distance is 2,847 Mpc...\")\n",
    "\n",
    "**This transforms Claude from a text generator into a research agent** that can:\n",
    "- Access real-time astronomical databases (SIMBAD, NED, Gaia)\n",
    "- Perform precise cosmological calculations\n",
    "- Execute data processing pipelines\n",
    "- Query your local files and instruments\n",
    "\n",
    "The key insight: **Claude decides when to use these tools**. You don't tell it \"use this function\"—it \n",
    "determines that itself based on the user's question. This is what makes it feel like working with an \n",
    "intelligent assistant rather than a simple API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cddd82e",
   "metadata": {},
   "source": [
    "## Understanding Function Tools\n",
    "\n",
    "Function tools represent a fundamental shift in how LLMs interact with your code. Until now, when you asked Claude to calculate something, it would describe the calculation process in text. You then had to implement that calculation yourself in Python. Function tools eliminate this middle step—Claude can now directly execute Python functions you've written.\n",
    "\n",
    "Think of it like the difference between having an assistant who can only read instruction manuals versus one who can actually operate the equipment. The first can tell you how to use a telescope; the second can actually point it at the stars and take measurements.\n",
    "\n",
    "Here's the key concept: you write Python functions using standard Python programming skills—NumPy arrays for calculations, file operations for data handling, and other familiar tools. Then you describe these functions to Claude in a special format called a \"function schema.\" Once Claude knows about your functions, it can call them directly when answering questions.\n",
    "\n",
    "**The Function Tool Workflow:**\n",
    "1. **You define**: Write a Python function using familiar Python tools\n",
    "2. **You describe**: Create a schema that tells Claude what the function does\n",
    "3. **User asks**: Someone poses a question requiring calculation\n",
    "4. **Claude decides**: Whether to use a function based on the question\n",
    "5. **Claude requests**: Tells you which function to run with what parameters\n",
    "6. **You execute**: Run the function and send results back\n",
    "7. **Claude interprets**: Incorporates the results into a natural language response\n",
    "\n",
    "When someone asks \"What's the distance to a star with 0.05 arcsecond parallax?\", Claude recognizes this requires calculation, requests your distance function with the parameter 0.05, and then explains the result in astronomical context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5e815",
   "metadata": {},
   "source": [
    "### The Schema Concept\n",
    "\n",
    "A function schema is like a user manual for your function—it tells Claude what the function does, what parameters it needs, and when to use it. Without a schema, Claude wouldn't know your function exists or how to use it.\n",
    "\n",
    "Think of schemas as the bridge between natural language and code. When someone asks \"What's the distance to Alpha Centauri if its parallax is 0.75 arcseconds?\", the schema helps Claude understand:\n",
    "- This question needs the `parallax_to_distance` function\n",
    "- The function needs one parameter: `parallax_arcsec`\n",
    "- The value for that parameter is 0.75\n",
    "\n",
    "The schema format might look complex at first, but it's just a structured way to describe what you'd tell a colleague about your function: what it does, what inputs it needs, and what outputs it provides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0086a69",
   "metadata": {},
   "source": [
    "## Your First Function Tool\n",
    "\n",
    "Let's create your first function tool step by step. We'll start with the simplest possible astronomical calculation and gradually build complexity.\n",
    "\n",
    "### Setting Up the Environment\n",
    "\n",
    "First, let's import what we need. Everything here should be familiar from previous lectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports from Part 1\n",
    "import numpy as np  # For mathematical operations\n",
    "import os          # For environment variables\n",
    "from dotenv import load_dotenv  # For loading API keys (from Part 1)\n",
    "import anthropic   # For talking to Claude (from Part 1)\n",
    "\n",
    "# Load API key from .env file (same as Part 1)\n",
    "load_dotenv()\n",
    "client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "print(\"✓ Environment ready for function tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d412c9",
   "metadata": {},
   "source": [
    "### Creating a Simple Astronomical Function\n",
    "\n",
    "Let's start with the most fundamental calculation in stellar astronomy: converting parallax to distance. The parallax of a star is the tiny angle it appears to shift when viewed from opposite sides of Earth's orbit. The smaller this angle, the farther away the star.\n",
    "\n",
    "The relationship is beautifully simple: distance (in parsecs) = 1 / parallax (in arcseconds). One parsec is the distance at which a star would have a parallax of exactly one arcsecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95db3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallax_to_distance(parallax_arcsec):\n",
    "    \"\"\"\n",
    "    Convert stellar parallax to distance in parsecs.\n",
    "    \n",
    "    The fundamental equation: d = 1/p\n",
    "    where d is distance in parsecs and p is parallax in arcseconds.\n",
    "    \"\"\"\n",
    "    # Input validation - always check for invalid inputs!\n",
    "    if parallax_arcsec <= 0:\n",
    "        return {\"error\": \"Parallax must be positive\"}\n",
    "    \n",
    "    # Calculate distance using the parallax formula\n",
    "    distance_pc = 1.0 / parallax_arcsec\n",
    "    \n",
    "    # Return as a dictionary for structured data\n",
    "    # We round to 2 decimal places for readability\n",
    "    return {\"distance_parsecs\": round(distance_pc, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec3db1",
   "metadata": {},
   "source": [
    "Let's test our function manually to make sure it works correctly. We'll use Proxima Centauri, our nearest stellar neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Proxima Centauri's parallax (0.768 arcsec)\n",
    "test_result = parallax_to_distance(0.768)\n",
    "print(f\"Distance to Proxima Centauri: {test_result['distance_parsecs']} parsecs\")\n",
    "print(f\"That's about {test_result['distance_parsecs'] * 3.26} light-years\")\n",
    "\n",
    "# Test error handling with invalid input\n",
    "error_test = parallax_to_distance(-1)\n",
    "print(f\"\\nError handling test: {error_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2681fa7",
   "metadata": {},
   "source": [
    "### Defining the Function Schema\n",
    "\n",
    "Now we need to tell Claude about our function. A schema describes three key things:\n",
    "1. **The function's name** - what Claude will call it\n",
    "2. **What it does** - helps Claude know when to use it\n",
    "3. **What inputs it needs** - the parameters and their types\n",
    "\n",
    "The schema uses a specific format that might look intimidating at first, but it's just a nested dictionary structure. Let's build it step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35807f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tools list with our function schema\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"parallax_to_distance\",  # The exact function name\n",
    "        \"description\": \"Calculate stellar distance from parallax measurement in arcseconds\",\n",
    "        \"input_schema\": {  # Describes what inputs the function needs\n",
    "            \"type\": \"object\",  # The inputs are structured as an object\n",
    "            \"properties\": {  # List of parameters\n",
    "                \"parallax_arcsec\": {  # Parameter name (must match function)\n",
    "                    \"type\": \"number\",  # This parameter is a number\n",
    "                    \"description\": \"Parallax angle in arcseconds (must be positive)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"parallax_arcsec\"]  # This parameter is mandatory\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✓ Function schema defined\")\n",
    "print(f\"Claude now knows about {len(tools)} function(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc385787",
   "metadata": {},
   "source": [
    "### Making Your First Function Tool Call\n",
    "\n",
    "Now for the exciting part—let's ask Claude a question and see if it recognizes that it needs to use our function. This is different from Part 1 because we're giving Claude the ability to request function execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeee79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Claude a question that requires our function\n",
    "message = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5\",\n",
    "    max_tokens=300,\n",
    "    tools=tools,  # NEW! This gives Claude access to our functions\n",
    "    messages=[{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the distance to a star with a parallax of 0.05 arcseconds?\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# The response type tells us what Claude wants to do\n",
    "print(f\"Claude's response type: {message.stop_reason}\")\n",
    "print(f\"Number of content blocks in response: {len(message.content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8926130",
   "metadata": {},
   "source": [
    "### Understanding the Tool Response Structure\n",
    "\n",
    "When Claude wants to use a tool, it doesn't just return text like in Part 1. Instead, it returns a structured response with multiple \"blocks.\" Some blocks contain text (Claude's thoughts), and some contain tool requests (functions Claude wants to run).\n",
    "\n",
    "Let's examine this structure carefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfe007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what Claude sent back\n",
    "if message.stop_reason == \"tool_use\":\n",
    "    print(\"Claude wants to use a function!\\n\")\n",
    "    \n",
    "    # Look at each block in the response\n",
    "    for i, block in enumerate(message.content):\n",
    "        print(f\"Block {i}: Type = '{block.type}'\")\n",
    "        \n",
    "        # Text blocks contain Claude's reasoning\n",
    "        if hasattr(block, 'text'):\n",
    "            print(f\"  Text content: \\\"{block.text}\\\"\")\n",
    "        \n",
    "        # Tool use blocks contain function requests\n",
    "        if hasattr(block, 'name'):\n",
    "            print(f\"  Function to call: {block.name}\")\n",
    "            print(f\"  Arguments to pass: {block.input}\")\n",
    "            print(f\"  Unique ID for this call: {block.id}\")\n",
    "else:\n",
    "    print(\"Claude responded with text only (no function needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b394fa",
   "metadata": {},
   "source": [
    "### Extracting the Tool Request\n",
    "\n",
    "Claude has told us it wants to use a function, but it hasn't actually run anything yet. We need to extract the tool request, execute our Python function, and send the result back. This gives us full control over what code actually runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tool use request is typically the last content block\n",
    "tool_use = message.content[-1]\n",
    "\n",
    "print(\"Tool request details:\")\n",
    "print(f\"  Function name: {tool_use.name}\")\n",
    "print(f\"  Arguments: {tool_use.input}\")\n",
    "print(f\"  Tool ID: {tool_use.id}\")\n",
    "print(\"\\nThis ID is important - we need it to send results back to Claude!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd3d0d",
   "metadata": {},
   "source": [
    "### Executing the Function\n",
    "\n",
    "Now we need to execute our function with the arguments Claude provided. Claude sends arguments as a dictionary like `{'parallax_arcsec': 0.05}`. We can extract the value and call our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ff9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute our function with Claude's arguments\n",
    "print(f\"Claude wants to call: {tool_use.name} with {tool_use.input}\")\n",
    "\n",
    "# Extract the parallax value from the dictionary\n",
    "parallax_value = tool_use.input['parallax_arcsec']\n",
    "print(f\"Extracted parallax value: {parallax_value}\")\n",
    "\n",
    "# Call our function\n",
    "result = parallax_to_distance(parallax_value)\n",
    "print(f\"Function result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7e22c",
   "metadata": {},
   "source": [
    "### Completing the Conversation with Natural Language\n",
    "\n",
    "This is a crucial step: we need to send the function result back to Claude so it can formulate a complete, natural language answer. Without this step, the user would just see raw function output instead of a helpful explanation. This is what transforms a simple calculation into a conversational response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1844c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation by sending the function result back to Claude\n",
    "final_response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5\",\n",
    "    max_tokens=200,\n",
    "    tools=tools,\n",
    "    messages=[\n",
    "        # The original user question\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What is the distance to a star with a parallax of 0.05 arcseconds?\"\n",
    "        },\n",
    "        # Claude's response requesting the function\n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": message.content\n",
    "        },\n",
    "        # Our function result sent back to Claude\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_use_id\": tool_use.id,  # Must match the original request ID\n",
    "                \"content\": str(result)  # Convert result to string\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Claude's final natural language answer:\")\n",
    "print(\"=\" * 50)\n",
    "print(final_response.content[0].text)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nNotice how Claude converts the raw number into a complete explanation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9cf2f4",
   "metadata": {},
   "source": [
    "## Building Multiple Astronomical Functions\n",
    "\n",
    "Now that you understand the complete workflow—from function definition to natural language response—let's expand your toolkit with more astronomical calculations. We'll see how Claude intelligently chooses between different functions based on the question.\n",
    "\n",
    "### Adding a Stellar Luminosity Calculator\n",
    "\n",
    "The Stefan-Boltzmann law tells us that a star's luminosity depends on its size and temperature. Specifically, L = 4πR²σT⁴, where σ is the Stefan-Boltzmann constant. This fundamental relationship lets us calculate how much energy a star emits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stellar_luminosity(radius_solar, temperature_k):\n",
    "    \"\"\"\n",
    "    Calculate stellar luminosity using the Stefan-Boltzmann law.\n",
    "    \n",
    "    The energy radiated by a star depends on its surface area (4πR²)\n",
    "    and how much energy each square meter emits (σT⁴).\n",
    "    \"\"\"\n",
    "    # Physical constants\n",
    "    stefan_boltzmann = 5.67e-8  # W m^-2 K^-4 (Stefan-Boltzmann constant)\n",
    "    solar_radius = 6.96e8  # meters (Sun's radius)\n",
    "    solar_luminosity = 3.83e26  # watts (Sun's total energy output)\n",
    "    \n",
    "    # Always validate inputs\n",
    "    if radius_solar <= 0 or temperature_k <= 0:\n",
    "        return {\"error\": \"Radius and temperature must be positive\"}\n",
    "    \n",
    "    # Convert stellar radius from solar units to meters\n",
    "    radius_meters = radius_solar * solar_radius\n",
    "    \n",
    "    # Apply Stefan-Boltzmann law: L = 4πR²σT⁴\n",
    "    luminosity_watts = 4 * np.pi * radius_meters**2 * stefan_boltzmann * temperature_k**4\n",
    "    \n",
    "    # Convert to solar luminosities for easier interpretation\n",
    "    luminosity_solar = luminosity_watts / solar_luminosity\n",
    "    \n",
    "    return {\n",
    "        \"luminosity_solar\": round(luminosity_solar, 3),\n",
    "        \"luminosity_watts\": f\"{luminosity_watts:.2e}\"  # Scientific notation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec554d8e",
   "metadata": {},
   "source": [
    "Let's verify our function works correctly by testing it with the Sun's values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a721a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the Sun (should give ~1.0 solar luminosity)\n",
    "sun_test = stellar_luminosity(1.0, 5778)  # Sun: 1 solar radius, 5778 K\n",
    "print(f\"Sun's calculated luminosity: {sun_test['luminosity_solar']} L☉\")\n",
    "print(f\"In watts: {sun_test['luminosity_watts']} W\")\n",
    "print(\"(Should be very close to 1.0 solar luminosity!)\")\n",
    "\n",
    "# Test with a red giant\n",
    "red_giant = stellar_luminosity(25, 3500)  # Typical red giant values\n",
    "print(f\"\\nRed giant luminosity: {red_giant['luminosity_solar']} L☉\")\n",
    "print(\"(Much brighter than the Sun despite being cooler, due to larger size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa9cba",
   "metadata": {},
   "source": [
    "### Updating the Tools List\n",
    "\n",
    "Now we need to tell Claude about both functions. Claude will automatically learn to choose the right function based on the question content—questions about distance will trigger the parallax function, while questions about brightness will trigger the luminosity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded tools list with both functions\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"parallax_to_distance\",\n",
    "        \"description\": \"Calculate stellar distance from parallax measurement\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"parallax_arcsec\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Parallax in arcseconds (must be positive)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"parallax_arcsec\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stellar_luminosity\", \n",
    "        \"description\": \"Calculate stellar luminosity from radius and temperature\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"radius_solar\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Stellar radius in solar radii\"\n",
    "                },\n",
    "                \"temperature_k\": {\n",
    "                    \"type\": \"number\", \n",
    "                    \"description\": \"Effective temperature in Kelvin\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"radius_solar\", \"temperature_k\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Claude now has access to {len(tools)} functions:\")\n",
    "for tool in tools:\n",
    "    print(f\"  • {tool['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511a48e",
   "metadata": {},
   "source": [
    "### Creating a Complete Tool Execution Helper\n",
    "\n",
    "Since we'll be executing tools frequently, let's create a helper function that handles the complete workflow from question to natural language answer. This will make our code cleaner, avoid repetition, and ensure we always get natural language responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool_and_respond(question, tools):\n",
    "    \"\"\"\n",
    "    Complete workflow: question → tool execution → natural language answer.\n",
    "    \n",
    "    This function handles the entire process we've been doing manually:\n",
    "    1. Send question to Claude\n",
    "    2. Execute requested function if needed\n",
    "    3. Get natural language response\n",
    "    \"\"\"\n",
    "    # Step 1: Ask Claude the question\n",
    "    initial_response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-5\",\n",
    "        max_tokens=300,\n",
    "        tools=tools,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "    \n",
    "    # Check if Claude wants to use a tool\n",
    "    if initial_response.stop_reason != \"tool_use\":\n",
    "        # No tool needed, return direct response\n",
    "        return initial_response.content[0].text\n",
    "    \n",
    "    # Step 2: Execute the requested function\n",
    "    tool_use = initial_response.content[-1]\n",
    "    \n",
    "    # Execute the appropriate function based on name\n",
    "    if tool_use.name == \"parallax_to_distance\":\n",
    "        # Extract the parallax value and call function\n",
    "        parallax = tool_use.input['parallax_arcsec']\n",
    "        result = parallax_to_distance(parallax)\n",
    "    elif tool_use.name == \"stellar_luminosity\":\n",
    "        # Extract both parameters and call function\n",
    "        radius = tool_use.input['radius_solar']\n",
    "        temp = tool_use.input['temperature_k']\n",
    "        result = stellar_luminosity(radius, temp)\n",
    "    else:\n",
    "        result = {\"error\": f\"Unknown function: {tool_use.name}\"}\n",
    "    \n",
    "    # Step 3: Send result back for natural language response\n",
    "    final_response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-5\",\n",
    "        max_tokens=300,\n",
    "        tools=tools,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": initial_response.content},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"tool_result\",\n",
    "                    \"tool_use_id\": tool_use.id,\n",
    "                    \"content\": str(result)\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return final_response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a447eb",
   "metadata": {},
   "source": [
    "Now let's test our complete workflow with different astronomical questions to see Claude choose the right tool and provide natural language answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5afab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different types of questions\n",
    "test_questions = [\n",
    "    \"What's the distance to Proxima Centauri if its parallax is 0.768 arcseconds?\",\n",
    "    \"Calculate the luminosity of Betelgeuse with radius 700 solar radii and temperature 3500 K\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\\nAnswer:\")\n",
    "    answer = execute_tool_and_respond(question, tools)\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8361f9b",
   "metadata": {},
   "source": [
    "## Introduction to RAG (Retrieval Augmented Generation)\n",
    "\n",
    "As an astronomer, you likely work with extensive documentation—research papers, observation logs, telescope manuals, data processing notes, and more. A common challenge: when you need to find specific information from your materials, you end up with multiple browser tabs open, using Ctrl+F repeatedly through different documents, trying to locate that one calculation method or observation protocol.\n",
    "\n",
    "This is exactly the problem that RAG (Retrieval Augmented Generation) solves. RAG combines document search with LLM reasoning, allowing you to ask questions like \"How do we calibrate the spectrograph?\" or \"What are the best practices for dark sky observations?\" and get comprehensive answers drawn directly from your own documentation and notes.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation. Think of it as giving Claude access to your research library—not just its general knowledge, but your specific papers, notes, and documentation.\n",
    "\n",
    "The process has three steps:\n",
    "\n",
    "1. **Retrieval**: Search through your documents to find relevant sections\n",
    "2. **Augmentation**: Add those relevant sections to your question as context\n",
    "3. **Generation**: Have Claude answer using both its knowledge and your specific materials\n",
    "\n",
    "Without RAG, if you ask Claude \"What did we learn about temperature parameters in Part 1?\", it can only give general information. With RAG, it can tell you exactly what YOUR tutorial materials say, with the specific examples and explanations from the course.\n",
    "\n",
    "### Understanding Markdown Files (.md)\n",
    "\n",
    "Before we work with our tutorial materials, let's understand what a markdown file is. You've likely encountered markdown before—every text cell in Jupyter notebooks uses markdown formatting!\n",
    "\n",
    "**What is a .md file?**\n",
    "A markdown file (with the extension .md) is a plain text file that uses simple symbols for formatting:\n",
    "- `#` for headers (like `# Title` or `## Section`)\n",
    "- `*` for italics and `**` for bold\n",
    "- Three backticks (```) for code blocks\n",
    "- `-` for bullet points\n",
    "\n",
    "The beauty of markdown is that it's human-readable even without rendering. You can open a .md file in any text editor (like Cursor, Notepad, or TextEdit) and read it easily.\n",
    "\n",
    "### Converting Jupyter Notebooks to Markdown with Jupytext\n",
    "\n",
    "Tutorial materials are often in Jupyter notebook format (.ipynb files), which contain code and text mixed with metadata and output. To make them searchable for RAG, we need to convert them to plain markdown.\n",
    "\n",
    "**Jupytext** is a tool that converts between different notebook formats. Think of it as a translator that can turn your .ipynb files into clean .md files. Here's how to use it:\n",
    "\n",
    "First, install Jupytext:\n",
    "\n",
    "```bash\n",
    "pip install jupytext\n",
    "```\n",
    "\n",
    "To convert your notebook files to markdown, you would use Jupytext from the terminal:\n",
    "\n",
    "```bash\n",
    "jupytext --to md LLM_API_Basics_STIG.ipynb\n",
    "```\n",
    "\n",
    "This creates a file called `LLM_API_Basics_STIG.md` in the same folder. The markdown file contains all your text cells and code cells from the notebook, but in a clean text format perfect for searching.\n",
    "\n",
    "For this tutorial, we've already converted Part 1 to markdown format, so we can work with it directly. Let's read this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pre-converted Part 1 file\n",
    "with open('LLM_API_Basics_STIG.md', 'r') as f:\n",
    "    part1_content = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2cac23",
   "metadata": {},
   "source": [
    "### Finding Section Headers in the Document\n",
    "\n",
    "Markdown uses `#` symbols for headers. In our tutorial file:\n",
    "- `#` marks the main title\n",
    "- `##` marks major sections\n",
    "- `###` marks subsections\n",
    "\n",
    "Let's find all the main topics covered in Part 1 using simple string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all main sections using string methods\n",
    "sections = []\n",
    "lines = part1_content.split('\\n')  # Split into individual lines\n",
    "\n",
    "for line in lines:\n",
    "    # Check if line starts with '## ' (main section header)\n",
    "    if line.startswith('## '):\n",
    "        # Remove the '## ' to get just the title\n",
    "        section_title = line[3:]  # Everything after '## '\n",
    "        sections.append(section_title)\n",
    "\n",
    "print(f\"Found {len(sections)} main sections in Part 1:\")\n",
    "print()\n",
    "for i, section in enumerate(sections[:8], 1):  # Show first 8\n",
    "    print(f\"  {i}. {section}\")\n",
    "if len(sections) > 8:\n",
    "    print(f\"  ... and {len(sections) - 8} more sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7058624",
   "metadata": {},
   "source": [
    "## Document Chunking\n",
    "\n",
    "### Why We Need to Chunk Documents\n",
    "\n",
    "Our Part 1 file contains tens of thousands of characters—that's enormous! Sending the entire document to Claude every time we ask a question would create three major problems:\n",
    "\n",
    "1. **Cost**: We'd be paying for all those characters as input tokens for every single question, even if we're only asking about one small topic\n",
    "2. **Relevance**: If you ask about \"API errors\", 95% of the document isn't relevant—it's about other topics like image processing or conversation management\n",
    "3. **Focus**: Claude performs better with focused, relevant context rather than being overwhelmed with unrelated information\n",
    "\n",
    "The solution is **document chunking**—breaking the large document into smaller, manageable pieces. Think of it like organizing a library: instead of reading every page of every book to answer a question, you first identify which chapter or section is most relevant.\n",
    "\n",
    "### Simple Section-Based Chunking\n",
    "\n",
    "The simplest chunking strategy is to split by section headers. Each section of the tutorial becomes its own searchable chunk. This works well for structured documents like tutorial materials where each section covers a specific topic.\n",
    "\n",
    "Let's implement this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sections(text):\n",
    "    \"\"\"\n",
    "    Split a document into chunks based on ## section headers.\n",
    "    \n",
    "    This function:\n",
    "    1. Finds all the ## headers in the text\n",
    "    2. Splits the document at these headers\n",
    "    3. Keeps each section as a separate chunk\n",
    "    4. Preserves the section header with its content\n",
    "    \"\"\"\n",
    "    # Split on section headers\n",
    "    # We use '\\n## ' to ensure we're splitting on headers at line starts\n",
    "    sections = text.split('\\n## ')\n",
    "    \n",
    "    chunks = []\n",
    "    for i, section in enumerate(sections):\n",
    "        # The first section doesn't have '## ' removed (it wasn't split)\n",
    "        if i == 0:\n",
    "            chunk_text = section\n",
    "        else:\n",
    "            # Add back the '## ' that was removed during split\n",
    "            chunk_text = '## ' + section\n",
    "        \n",
    "        # Only keep chunks with substantial content (at least 100 characters)\n",
    "        if len(chunk_text.strip()) > 100:\n",
    "            chunks.append({\n",
    "                'text': chunk_text.strip(),\n",
    "                'length': len(chunk_text),\n",
    "                'chunk_id': i\n",
    "            })\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254da895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks from our tutorial\n",
    "lecture_chunks = chunk_by_sections(part1_content)\n",
    "\n",
    "print(f\"Created {len(lecture_chunks)} chunks from Part 1\")\n",
    "print(f\"\\nChunk statistics:\")\n",
    "print(f\"  Average size: {sum(c['length'] for c in lecture_chunks) // len(lecture_chunks):,} characters\")\n",
    "print(f\"  Smallest: {min(c['length'] for c in lecture_chunks):,} characters\")\n",
    "print(f\"  Largest: {max(c['length'] for c in lecture_chunks):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bbade",
   "metadata": {},
   "source": [
    "Let's examine what our chunks look like to understand what we've created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first few chunks\n",
    "print(\"First 3 chunks from Part 1:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(min(3, len(lecture_chunks))):\n",
    "    chunk = lecture_chunks[i]\n",
    "    # Get the first line (usually the section title)\n",
    "    first_line = chunk['text'].split('\\n')[0]\n",
    "    \n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Title: {first_line}\")\n",
    "    print(f\"  Size: {chunk['length']:,} characters\")\n",
    "    print(f\"  Preview: {chunk['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc02a62",
   "metadata": {},
   "source": [
    "### Understanding Overlapping Chunks\n",
    "\n",
    "A potential problem with simple splitting: what if important information spans across chunk boundaries? Imagine reading a textbook where each chapter ends mid-sentence—you'd lose crucial context!\n",
    "\n",
    "**Overlapping chunks** solve this by having each chunk include some content from its neighbors. It's like having each chapter of a book reprint the last paragraph of the previous chapter and the first paragraph of the next chapter. This ensures nothing important gets lost in the gaps between chunks.\n",
    "\n",
    "Here's a visual example:\n",
    "```\n",
    "Original text: \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "Non-overlapping chunks of size 10:\n",
    "  Chunk 1: \"ABCDEFGHIJ\"\n",
    "  Chunk 2: \"KLMNOPQRST\"\n",
    "  Chunk 3: \"UVWXYZ\"\n",
    "  \n",
    "Overlapping chunks (size 10, overlap 3):\n",
    "  Chunk 1: \"ABCDEFGHIJ\"\n",
    "  Chunk 2: \"HIJKLMNOPQ\"  (starts at H, overlaps HIJ)\n",
    "  Chunk 3: \"OPQRSTUVWX\"  (starts at O, overlaps OPQ)\n",
    "```\n",
    "\n",
    "While overlapping chunks are more sophisticated and useful for many applications, for our tutorial materials that are already well-structured with clear section boundaries, simple section-based chunking works well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cf1ab",
   "metadata": {},
   "source": [
    "## Understanding Embeddings\n",
    " \n",
    "Now we have chunks of text, but how do we find which chunks are relevant to a user's question? We can't just search for exact word matches—what if someone asks about \"error handling\" but the text says \"exception management\"? These mean the same thing but use different words.\n",
    " \n",
    "This is where **embeddings** come in. An embedding is a way to convert text into a list of numbers (called a vector) that captures the semantic meaning of that text. The key insight: texts with similar meanings will have similar number patterns, even if they use different words.\n",
    " \n",
    "Think of it like this:\n",
    "- \"stellar parallax\" might become [0.2, -0.1, 0.8, 0.3, ..., 0.5] (384 numbers)\n",
    "- \"star distance measurement\" might become [0.3, -0.2, 0.7, 0.4, ..., 0.4] (384 numbers)\n",
    "- \"cooking recipes\" might become [0.9, 0.5, -0.3, 0.1, ..., -0.2] (384 numbers)\n",
    " \n",
    "Notice how the first two (both about measuring star distances) have similar number patterns, while the third (about cooking) is completely different. The embedding model has learned that \"parallax\" and \"distance measurement\" are related concepts in astronomy.\n",
    " \n",
    "### How Embeddings Capture Meaning\n",
    " \n",
    "Embedding models are neural networks trained on millions of documents. Through this training, they learn:\n",
    "- \"API\" and \"programming interface\" are related concepts\n",
    "- \"error\" and \"exception\" often mean similar things in programming\n",
    "- \"temperature\" in the context of LLMs is different from \"temperature\" in physics\n",
    " \n",
    "Each dimension in the embedding vector captures some aspect of meaning. While we can't interpret what each individual number means (they're learned by the neural network), we can measure how similar two embeddings are to find related texts.\n",
    " \n",
    "### Important Note: Normalized Embeddings\n",
    " \n",
    "Most modern embedding models, including the one we'll use, output **normalized vectors**. This means all embedding vectors have a magnitude (length) of 1.0. This is a crucial property that simplifies our calculations significantly!\n",
    " \n",
    "### Important Tip: Complete Sentences Give Better Embeddings\n",
    " \n",
    "When creating embeddings, **complete sentences often work better than keywords!** The embedding model can better understand context and meaning from full sentences. For example:\n",
    "- \"How to measure stellar parallax?\" gives richer embeddings than just \"parallax\"\n",
    "- \"What are the error handling techniques in Python?\" is better than \"error handling\"\n",
    " \n",
    "This is because the model was trained on natural language text, so it better understands the relationships between words when they appear in complete thoughts.\n",
    " \n",
    "### Measuring Similarity with Cosine Similarity\n",
    " \n",
    "Once we have embeddings (vectors of numbers), we need to measure how similar they are. We use **cosine similarity**, which measures the angle between two vectors.\n",
    " \n",
    "The intuition is simple:\n",
    "- Vectors pointing in the same direction = similar meaning (cosine similarity ≈ 1)\n",
    "- Vectors at right angles = unrelated (cosine similarity ≈ 0)\n",
    "- Vectors pointing opposite ways = opposite meanings (cosine similarity ≈ -1)\n",
    " \n",
    "The mathematical formula is: \n",
    " \n",
    "$$\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}|| \\times ||\\mathbf{B}||}$$\n",
    " \n",
    "Where:\n",
    "- $\\mathbf{A} \\cdot \\mathbf{B}$ is the dot product (measures alignment)\n",
    "- $||\\mathbf{A}||$ and $||\\mathbf{B}||$ are the vector magnitudes (lengths)\n",
    " \n",
    "**However, since embedding models output normalized vectors ($||\\mathbf{A}|| = ||\\mathbf{B}|| = 1$), the formula simplifies to just the dot product:**\n",
    " \n",
    "$$\\cos(\\theta) = \\mathbf{A} \\cdot \\mathbf{B}$$\n",
    " \n",
    "Let's implement the simplified version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity for normalized vectors.\n",
    "    Since ||vec1|| = ||vec2|| = 1, cosine similarity = dot product.\n",
    "    Much faster and simpler!\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9a756",
   "metadata": {},
   "source": [
    "### Using the Sentence-Transformers Library\n",
    "\n",
    "To create actual embeddings that capture semantic meaning, we'll use a library called **sentence-transformers**. This library provides pre-trained neural network models that can convert any text into meaningful embedding vectors.\n",
    "\n",
    "**What does sentence-transformers do?**\n",
    "- Provides ready-to-use embedding models trained on millions of documents\n",
    "- Handles all the complex neural network operations behind the scenes\n",
    "- Converts text to vectors that actually capture semantic meaning\n",
    "- Works with sentences, paragraphs, or entire documents\n",
    "\n",
    "We'll use a model called **'all-MiniLM-L6-v2'** for this tutorial. Breaking down this name helps understand what we're working with: \"all\" means it works for all types of English text, \"MiniLM\" indicates it's a smaller, faster version of a language model, \"L6\" tells us it has 6 layers (the depth of the neural network), and \"v2\" simply means it's version 2, improved from the original.\n",
    "\n",
    "**Importantly, this model outputs normalized vectors**, so we can use the simplified cosine similarity calculation.\n",
    "\n",
    "This model converts any text into 384 numbers that capture its meaning. Through its training, it has learned that phrases like \"stellar distance\" and \"how far away is the star\" mean similar things, even though they use different words. We're using this particular model because it strikes the perfect balance for learning—it's small enough to run quickly on any computer (only 80MB download), fast enough for interactive experimentation, and powerful enough to demonstrate all RAG concepts effectively.\n",
    "\n",
    "**More Powerful Models in Production**\n",
    "\n",
    "In professional research and production systems, you'll often encounter more sophisticated embedding models. For example, OpenAI's text-embedding-3-large creates 3,072-dimensional embeddings compared to our 384, providing much richer semantic understanding. Google's text-embedding-004 produces 768-dimensional embeddings with excellent multilingual support. Specialized models like Voyage AI's voyage-3 or Cohere's embed-v3 offer 1,024 dimensions optimized for domain-specific or technical texts.\n",
    "\n",
    "These larger models can capture more subtle semantic relationships and often perform better with specialized scientific literature. However, they come with trade-offs: they're more expensive (often requiring API payments), slower to run, require significantly more memory and storage, and are honestly overkill for learning the fundamental concepts.\n",
    "\n",
    "Think of it like choosing a telescope: our all-MiniLM-L6-v2 is like a reliable 8-inch telescope that's perfect for learning astronomy. The production models are like research-grade observatories—more powerful, but you don't need them to understand how telescopes work! For your course projects and learning RAG concepts, our smaller model is perfectly adequate. When you eventually move to research-scale projects with thousands of papers, you can upgrade to these more powerful models using the exact same techniques you're learning today.\n",
    "\n",
    "You can install the sentence-transformers library with the following command:\n",
    "\n",
    "```bash\n",
    "pip install -q sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f911821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "print(\"(First time will download ~80MB model file)\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "# Explore model properties\n",
    "print(f\"\\nModel information:\")\n",
    "print(f\"  Output dimensions: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Max input length: {embedding_model.max_seq_length} tokens\")\n",
    "print(f\"  (A token is roughly a word or word piece)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355bb27",
   "metadata": {},
   "source": [
    "Let's test the embedding model to see how it captures semantic meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a179f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with astronomy concepts\n",
    "test_texts = [\n",
    "    \"stellar parallax measurement\",\n",
    "    \"measuring star distances\",  # Similar meaning, different words\n",
    "    \"galaxy classification\",      # Different astronomy topic\n",
    "    \"cooking recipes\"             # Completely unrelated\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Creating embeddings for test phrases...\")\n",
    "test_embeddings = []\n",
    "for text in test_texts:\n",
    "    embedding = embedding_model.encode(text)\n",
    "    test_embeddings.append(embedding)\n",
    "    print(f\"  '{text}': vector with {len(embedding)} dimensions\")\n",
    "\n",
    "# Verify that embeddings are normalized\n",
    "print(\"\\nChecking if embeddings are normalized:\")\n",
    "for text, embedding in zip(test_texts, test_embeddings):\n",
    "    norm = np.linalg.norm(embedding)\n",
    "    print(f\"  '{text}': norm = {norm:.4f}\")\n",
    "\n",
    "print(\"\\n✓ All embeddings are normalized! We can use the simplified dot product for similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities between all pairs\n",
    "print(\"\\nSemantic similarities between phrases:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(len(test_texts)):\n",
    "    for j in range(i+1, len(test_texts)):\n",
    "        sim = cosine_similarity(test_embeddings[i], test_embeddings[j])\n",
    "        print(f\"'{test_texts[i]}' vs '{test_texts[j]}'\")\n",
    "        print(f\"  Similarity: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nNotice: 'stellar parallax' and 'star distances' have HIGH similarity!\")\n",
    "print(\"The model understands they're about the same concept.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f86fd",
   "metadata": {},
   "source": [
    "## Building the Complete RAG System\n",
    "\n",
    "Now let's combine everything we've learned to build a complete RAG system. We'll create embeddings for all our lecture chunks, build a search function that finds relevant content, and use that content to answer questions.\n",
    "\n",
    "### Step 1: Create Embeddings for All Chunks\n",
    "\n",
    "First, we need to convert every chunk of our tutorial into an embedding vector. This is like creating an index for a book—we're preparing the content to be efficiently searchable. Each chunk gets converted to 384 numbers that capture its meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3120f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all tutorial chunks\n",
    "print(f\"Creating embeddings for {len(lecture_chunks)} chunks...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "chunk_embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(lecture_chunks):\n",
    "    # Create embedding for this chunk's text\n",
    "    # The encode() method converts text to a vector\n",
    "    embedding = embedding_model.encode(chunk['text'])\n",
    "    chunk_embeddings.append(embedding)\n",
    "    \n",
    "    # Show progress every 5 chunks\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(lecture_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n✓ Created {len(chunk_embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {len(chunk_embeddings[0])} dimensions\")\n",
    "print(f\"Total data: {len(chunk_embeddings)} chunks × {len(chunk_embeddings[0])} dimensions = {len(chunk_embeddings) * len(chunk_embeddings[0]):,} numbers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afadd27",
   "metadata": {},
   "source": [
    "### Step 2: Building the Search Function\n",
    "\n",
    "Now we can build a search function that finds the most relevant chunks for any question. This is the \"Retrieval\" part of RAG. The process is:\n",
    "1. Convert the user's question to an embedding\n",
    "2. Compare it with all chunk embeddings using cosine similarity\n",
    "3. Return the chunks with the highest similarity scores\n",
    "\n",
    "This is like having a librarian who understands meaning, not just keywords. If you ask about \"error handling\", it will find sections about \"exceptions\" and \"try-except blocks\" even if they don't use the exact phrase \"error handling\".\n",
    "\n",
    "We'll use a vectorized approach for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c013bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Find the most relevant chunks for a query using vectorized operations.\n",
    "    \n",
    "    This function:\n",
    "    1. Converts the query to an embedding (384 numbers)\n",
    "    2. Calculates similarity with all chunk embeddings using vectorized NumPy\n",
    "    3. Returns the top-k most similar chunks\n",
    "    \n",
    "    Parameters:\n",
    "    - query: The search question\n",
    "    - top_k: How many results to return\n",
    "    \"\"\"\n",
    "    # Convert query to embedding (same 384-dimensional space as chunks)\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    # Vectorized similarity calculation - much faster than a loop!\n",
    "    # Convert list of embeddings to NumPy array for vectorized operations\n",
    "    chunk_matrix = np.array(chunk_embeddings)\n",
    "    \n",
    "    # Calculate dot products with all chunks at once\n",
    "    similarities = np.dot(chunk_matrix, query_embedding)\n",
    "    \n",
    "    # Find the indices of top-k highest similarities\n",
    "    # argsort() returns indices that would sort the array\n",
    "    # [-top_k:] takes the last k elements (highest values)\n",
    "    # [::-1] reverses to get descending order\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Return the top chunks with their similarities\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': lecture_chunks[idx],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c98a44",
   "metadata": {},
   "source": [
    "### Step 3: Testing the Search\n",
    "\n",
    "Let's test our search function with a specific question about API security from Part 1. This will show us which sections of the tutorial are most relevant to our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search with a specific question\n",
    "query = \"How do I keep API keys secure?\"\n",
    "results = search_chunks(query, top_k=2)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Found {len(results)} relevant sections:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    # Extract section title (first line)\n",
    "    lines = result['chunk']['text'].split('\\n')\n",
    "    title = lines[0] if lines else \"No title\"\n",
    "    \n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Similarity score: {result['similarity']:.3f}\")\n",
    "    print(f\"  (1.0 = perfect match, 0.0 = unrelated)\")\n",
    "    print(f\"  Section: {title}\")\n",
    "    print(f\"  Preview: {result['chunk']['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264ec42",
   "metadata": {},
   "source": [
    "### Step 4: RAG-Powered Question Answering\n",
    " \n",
    "Now for the complete RAG workflow. We'll create a function that combines everything:\n",
    " \n",
    "1. **Retrieval**: Search for relevant chunks from our tutorial materials using semantic similarity\n",
    "2. **Augmentation**: Add the retrieved content to our prompt as context for the AI\n",
    "3. **Generation**: Use Claude to generate an answer based on the retrieved information\n",
    " \n",
    "The `rag_answer()` function below implements this complete pipeline:\n",
    " \n",
    "- **Input**: Takes a question and optionally the number of chunks to retrieve\n",
    "- **Retrieval Step**: Uses our `search_chunks()` function to find the most relevant sections\n",
    "- **Quality Check**: Filters out results with low similarity scores (< 0.2) to avoid irrelevant content\n",
    "- **Smart Augmentation**: Combines retrieved chunks but ensures we end at complete sentences (no cut-off mid-sentence)\n",
    "- **Prompt Engineering**: Creates a structured prompt that includes both the question and retrieved tutorial materials\n",
    "- **Generation**: Sends the augmented prompt to Claude with low temperature (0.0) for factual accuracy\n",
    "- **Output**: Returns an answer grounded in our actual tutorial materials\n",
    " \n",
    "This approach ensures the AI answers questions using specific information from our tutorial materials, rather than just relying on its general training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bafd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(question, max_chunks=2):\n",
    "    \"\"\"\n",
    "    Answer a question using RAG (Retrieval Augmented Generation).\n",
    "    \n",
    "    Improved version that doesn't cut off mid-sentence!\n",
    "    \n",
    "    The three RAG steps:\n",
    "    1. RETRIEVAL: Find relevant chunks from tutorial materials\n",
    "    2. AUGMENTATION: Add those chunks to the prompt\n",
    "    3. GENERATION: Get Claude to answer using the retrieved content\n",
    "    \"\"\"\n",
    "    print(f\"Searching for content related to: '{question}'\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    results = search_chunks(question, top_k=max_chunks)\n",
    "    \n",
    "    # Check if we found relevant content\n",
    "    if results[0]['similarity'] < 0.2:\n",
    "        return \"No relevant content found in tutorial materials for this question.\"\n",
    "    \n",
    "    print(f\"Found {len(results)} relevant sections (similarity > 0.2)\")\n",
    "    \n",
    "    # Step 2: Augment - combine retrieved chunks \n",
    "    context_parts = []\n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Take more content but end at a complete sentence\n",
    "        chunk_text = result['chunk']['text'][:1500]  # Take up to 1500 chars\n",
    "        \n",
    "        # Find the last period, question mark, or exclamation point\n",
    "        # to end at a complete sentence\n",
    "        last_sentence_end = max(\n",
    "            chunk_text.rfind('.'),\n",
    "            chunk_text.rfind('?'),\n",
    "            chunk_text.rfind('!')\n",
    "        )\n",
    "        \n",
    "        if last_sentence_end > 0:\n",
    "            chunk_text = chunk_text[:last_sentence_end + 1]\n",
    "        \n",
    "        context_parts.append(f\"Section {i}:\\n{chunk_text}\")\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Create augmented prompt with retrieved content\n",
    "    augmented_prompt = f\"\"\"Based on the following tutorial materials from Part 1, answer this question: {question}\n",
    "\n",
    "TUTORIAL MATERIALS:\n",
    "{context}\n",
    "\n",
    "Please provide a comprehensive answer based specifically on what the tutorial materials say. Use the exact terminology and examples from the tutorial.\"\"\"\n",
    "    \n",
    "    # Step 3: Generate answer with Claude\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-5\",\n",
    "        max_tokens=400,\n",
    "        temperature=0.0,  # Low temperature for factual accuracy\n",
    "        messages=[{\"role\": \"user\", \"content\": augmented_prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e530068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG answering with a question about tutorial content\n",
    "test_question = \"What are the main parameters for API calls we learned about?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RAG-POWERED ANSWER\")\n",
    "print(\"=\" * 70)\n",
    "answer = rag_answer(test_question)\n",
    "print(f\"\\nAnswer based on Part 1 content:\")\n",
    "print(answer)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a040c92",
   "metadata": {},
   "source": [
    "### Comparing RAG vs Non-RAG Responses\n",
    "\n",
    "Let's see the dramatic difference between Claude's general knowledge and answers grounded in your specific tutorial materials. This demonstrates why RAG is so powerful for working with your own documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_question = \"What did we learn about conversation histories in the API?\"\n",
    "\n",
    "# Without RAG - just Claude's general knowledge\n",
    "print(\"WITHOUT Tutorial Materials (General Knowledge):\")\n",
    "print(\"=\" * 50)\n",
    "general_response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5\",\n",
    "    max_tokens=200,\n",
    "    messages=[{\"role\": \"user\", \"content\": comparison_question}]\n",
    ")\n",
    "print(general_response.content[0].text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "# With RAG - using tutorial materials\n",
    "print(\"WITH Tutorial Materials (RAG-Enhanced):\")\n",
    "print(\"=\" * 50)\n",
    "rag_answer_text = rag_answer(comparison_question)\n",
    "print(rag_answer_text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nNotice: The RAG answer references specific details from YOUR tutorial!\")\n",
    "print(\"It mentions the exact concepts and examples we covered in Part 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94655b",
   "metadata": {},
   "source": [
    "## Combining Function Tools with RAG\n",
    "\n",
    "Now for the grand finale—let's combine our calculation functions with document search to create a complete AI assistant. This assistant can both compute astronomical values and search your tutorial materials, choosing the right tool for each question.\n",
    "\n",
    "This combination is powerful: imagine asking \"What's the distance to a star with 0.1 arcsec parallax, and what did we learn about parallax in Part 1?\" The assistant can calculate the distance AND find relevant tutorial content.\n",
    "\n",
    "### Creating a Search Function for Claude\n",
    "\n",
    "First, let's wrap our RAG search in a function that Claude can call as a tool. This version properly handles complete sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a60d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_course_materials(question, max_results=2):\n",
    "    \"\"\"\n",
    "    Search tutorial materials and return relevant content.\n",
    "    This function will be callable by Claude as a tool.\n",
    "    \"\"\"\n",
    "    # Search for relevant chunks\n",
    "    results = search_chunks(question, top_k=max_results)\n",
    "    \n",
    "    # Check if we found anything relevant\n",
    "    if results[0]['similarity'] < 0.2:\n",
    "        return {\n",
    "            \"status\": \"no_relevant_content\",\n",
    "            \"message\": \"No relevant tutorial material found for this question\"\n",
    "        }\n",
    "    \n",
    "    # Format results for Claude\n",
    "    content_parts = []\n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Get section title\n",
    "        lines = result['chunk']['text'].split('\\n')\n",
    "        title = lines[0] if lines else \"No title\"\n",
    "        \n",
    "        # Get content ending at complete sentence\n",
    "        content_text = result['chunk']['text'][:1000]\n",
    "        last_period = content_text.rfind('.')\n",
    "        if last_period > 0:\n",
    "            content_text = content_text[:last_period + 1]\n",
    "        \n",
    "        content_parts.append(f\"Section {i} - {title}:\\n{content_text}\")\n",
    "    \n",
    "    # Return structured results\n",
    "    return {\n",
    "        \"status\": \"found\",\n",
    "        \"best_similarity\": round(results[0]['similarity'], 3),\n",
    "        \"content\": \"\\n\\n\".join(content_parts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fc91a",
   "metadata": {},
   "source": [
    "### Complete Tool Set with Calculations and Search\n",
    "\n",
    "Now let's create our complete tool set that combines astronomical calculations with tutorial material search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da333e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete tools list combining calculations and search\n",
    "complete_tools = [\n",
    "    {\n",
    "        \"name\": \"parallax_to_distance\",\n",
    "        \"description\": \"Calculate stellar distance from parallax measurement\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"parallax_arcsec\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Parallax in arcseconds\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"parallax_arcsec\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stellar_luminosity\",\n",
    "        \"description\": \"Calculate stellar luminosity from radius and temperature\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"radius_solar\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Radius in solar radii\"\n",
    "                },\n",
    "                \"temperature_k\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Temperature in Kelvin\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"radius_solar\", \"temperature_k\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_course_materials\",\n",
    "        \"description\": \"Search Part 1 tutorial notes for relevant content\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"question\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Topic or question to search for\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results (default 2)\",\n",
    "                    \"default\": 2\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"question\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Complete AI Assistant with {len(complete_tools)} capabilities:\")\n",
    "for tool in complete_tools:\n",
    "    print(f\"  • {tool['name']}: {tool['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8308c5e",
   "metadata": {},
   "source": [
    "### Complete Assistant Function with Natural Language Responses\n",
    "\n",
    "Let's create a complete assistant function that handles the entire workflow, ensuring we always get natural language answers whether Claude uses calculations or searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_assistant(question):\n",
    "    \"\"\"\n",
    "    Complete AI assistant that can calculate and search.\n",
    "    Always returns a natural language answer.\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {question}\")\n",
    "\n",
    "    # Get Claude's initial response\n",
    "    initial_response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-5\",\n",
    "        max_tokens=300,\n",
    "        tools=complete_tools,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "\n",
    "    # Check if Claude needs a tool\n",
    "    if initial_response.stop_reason != \"tool_use\":\n",
    "        # Some SDKs return plain dicts when running offline; fall back to repr\n",
    "        # repr() returns a string representation of the object for debugging\n",
    "        text_blocks = [\n",
    "            getattr(block, \"text\", None)\n",
    "            if not isinstance(block, dict)\n",
    "            else block.get(\"text\")\n",
    "            for block in getattr(initial_response, \"content\", [])\n",
    "        ]\n",
    "        text_blocks = [item for item in text_blocks if item]\n",
    "        return \"\".join(text_blocks).strip() or repr(initial_response)\n",
    "\n",
    "    # Execute the requested tool\n",
    "    tool_use = initial_response.content[-1]\n",
    "    print(f\"  → Using tool: {tool_use.name}\")\n",
    "\n",
    "    # Execute the appropriate function\n",
    "    if tool_use.name == \"parallax_to_distance\":\n",
    "        result = parallax_to_distance(tool_use.input['parallax_arcsec'])\n",
    "    elif tool_use.name == \"stellar_luminosity\":\n",
    "        result = stellar_luminosity(\n",
    "            tool_use.input['radius_solar'],\n",
    "            tool_use.input['temperature_k']\n",
    "        )\n",
    "    elif tool_use.name == \"search_course_materials\":\n",
    "        result = search_course_materials(\n",
    "            tool_use.input['question'],\n",
    "            tool_use.input.get('max_results', 2)\n",
    "        )\n",
    "    else:\n",
    "        result = {\"error\": f\"Unknown function: {tool_use.name}\"}\n",
    "\n",
    "    # Get natural language response\n",
    "    final_response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-5\",\n",
    "        max_tokens=400,\n",
    "        tools=complete_tools,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": initial_response.content},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"tool_result\",\n",
    "                    \"tool_use_id\": tool_use.id,\n",
    "                    \"content\": str(result)\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    text_blocks = [\n",
    "        getattr(block, \"text\", None)\n",
    "        if not isinstance(block, dict)\n",
    "        else block.get(\"text\")\n",
    "        for block in getattr(final_response, \"content\", [])\n",
    "    ]\n",
    "    text_blocks = [item for item in text_blocks if item]\n",
    "    if text_blocks:\n",
    "        return \"\".join(text_blocks).strip()\n",
    "\n",
    "    # As a fallback, provide the raw response so readers know to run locally\n",
    "    return repr(final_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b64614",
   "metadata": {},
   "source": [
    "### Testing the Complete System\n",
    "\n",
    "Let's test our complete AI assistant with different types of questions—calculations, course content searches, and general questions. Notice how Claude automatically chooses the right tool and provides natural language answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea068dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different types of questions\n",
    "test_scenarios = [\n",
    "    \"What's the distance to a star with 0.1 arcsecond parallax?\",\n",
    "    \"What did we learn about conversation histories in the API?\",\n",
    "    \"Calculate the luminosity of a star with radius 3 solar radii and temperature 7000K\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(\"=\" * 60)\n",
    "    answer = complete_assistant(question)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f17f54",
   "metadata": {},
   "source": [
    "## Vector Databases - The Professional Solution\n",
    "\n",
    "What we've built today is a fully functional RAG system that works well for single documents or small collections. However, when you're dealing with thousands of documents or millions of chunks in professional research, you need more sophisticated tools called **vector databases**.\n",
    "\n",
    "Vector databases are specialized systems designed to store and search embeddings efficiently. They're like regular databases, but optimized for finding similar vectors quickly, even when you have billions of them.\n",
    "\n",
    "### Three Popular Vector Database Solutions\n",
    "\n",
    "Here are three of the most popular vector database solutions you're likely to encounter:\n",
    "\n",
    "**1. Chroma** - Perfect for getting started\n",
    "Chroma is an open-source, completely free vector database that works seamlessly with Python. It can run entirely in memory for small projects, making it ideal for prototyping and learning. The API feels natural after today's lecture—you'll find the transition straightforward.\n",
    "\n",
    "**2. Pinecone** - The managed cloud solution\n",
    "Pinecone offers a fully managed cloud service where you don't need to maintain any servers. It handles scaling automatically as your data grows, making it more expensive but very reliable and fast. Many production AI applications use Pinecone when they need enterprise-level reliability without the hassle of infrastructure management.\n",
    "\n",
    "**3. FAISS** - Facebook's high-performance library\n",
    "Developed by Facebook AI Research, FAISS is extremely fast, especially with GPU acceleration. It's more of a library than a full database, but when speed is absolutely critical and you need to handle billions of vectors efficiently, FAISS is often the go-to choice.\n",
    "\n",
    "### When to Use Vector Databases\n",
    "\n",
    "Our implementation today works great for single documents or small collections (under 100 documents), prototyping and learning RAG concepts, and understanding how semantic search works under the hood.\n",
    "\n",
    "You should consider upgrading to a vector database when you're working with thousands of documents or research papers, need persistent storage with embeddings saved to disk, have multiple users searching simultaneously, want advanced features like filtering and metadata search, or are building production applications for research teams.\n",
    "\n",
    "### Working Example: ChromaDB\n",
    "\n",
    "Let's see how easy it is to upgrade our system to use ChromaDB. ChromaDB is a vector database that handles storage and search for us, though there are a few important differences from our manual implementation:\n",
    "\n",
    "**Key Differences to Note:**\n",
    "1. **ChromaDB uses its own default embedding model** (not our `all-MiniLM-L6-v2`) unless you explicitly override it\n",
    "2. **ChromaDB returns distances, not similarities** - lower values mean more similar\n",
    "\n",
    "Here's the implementation:\n",
    "\n",
    "First, install ChromaDB:\n",
    "\n",
    "```bash\n",
    "pip install -q chromadb\n",
    "```\n",
    "\n",
    "Then, import ChromaDB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Read the same Part 1 file\n",
    "with open('LLM_API_Basics_STIG.md', 'r') as f:\n",
    "    part1_content = f.read()\n",
    "\n",
    "# Use our same chunking function\n",
    "lecture_chunks = chunk_by_sections(part1_content)\n",
    "\n",
    "# Create ChromaDB client and clean up any existing collection\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Delete the collection if it already exists\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"part1_rag\")\n",
    "    print(\"Deleted existing collection\")\n",
    "except:\n",
    "    print(\"No existing collection to delete\")\n",
    "\n",
    "# Create new collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"part1_rag\",\n",
    "    metadata={\"description\": \"Part 1 content for RAG\"}\n",
    ")\n",
    "\n",
    "# Add all chunks to ChromaDB (it handles embeddings automatically!)\n",
    "for i, chunk in enumerate(lecture_chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk['text']],\n",
    "        ids=[f\"chunk_{i}\"],\n",
    "        metadatas=[{\"chunk_id\": i, \"length\": chunk['length']}]\n",
    "    )\n",
    "\n",
    "print(f\"Added {len(lecture_chunks)} chunks to ChromaDB\")\n",
    "\n",
    "# Now search is incredibly simple\n",
    "results = collection.query(\n",
    "    query_texts=[\"How do I keep API keys secure?\"],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "    print(f\"\\nResult {i+1} (distance: {distance:.3f}):\")\n",
    "    print(doc[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e03c7b",
   "metadata": {},
   "source": [
    "That's it! Notice how ChromaDB:\n",
    "- Automatically creates embeddings using the same model\n",
    "- Stores everything persistently (survives restarts)\n",
    "- Handles all the vector similarity calculations\n",
    "- Returns results ranked by relevance\n",
    "- Can store metadata alongside each chunk\n",
    "\n",
    "The concepts are identical to what we built—ChromaDB just handles the infrastructure for us. You could now search through hundreds of research papers, observation logs, or documentation files without changing the code!\n",
    "\n",
    "What matters isn't the specific vector database you use, but understanding the concepts we've covered today. Documents get chunked into manageable pieces, chunks get converted to embeddings, queries get converted to embeddings, similarity search finds relevant chunks, and retrieved content augments LLM prompts. With this understanding, you can use any vector database—they're all just different implementations of the same core ideas you've mastered today!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb577c3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed this two-part tutorial series on building LLM-powered research agents. \n",
    "You now have the skills to create sophisticated AI assistants for astronomical research.\n",
    "\n",
    "**In Part 1, you learned:**\n",
    "- API fundamentals: authentication, requests, parameters\n",
    "- Building conversations and managing context\n",
    "- Prompting strategies and structured outputs\n",
    "- Vision models for image analysis\n",
    "\n",
    "**In Part 2 (this lecture), you mastered:**\n",
    "\n",
    "**Function Tools:**\n",
    "- Creating function schemas that Claude can understand\n",
    "- Building astronomical calculation tools (luminosity distance, magnitude conversions)\n",
    "- Implementing the complete tool-use workflow\n",
    "- Managing multiple tools and letting Claude choose the right one\n",
    "\n",
    "**Retrieval Augmented Generation (RAG):**\n",
    "- Document chunking strategies with overlap\n",
    "- Computing embeddings for semantic search\n",
    "- Building search functions to find relevant information\n",
    "- Combining retrieval with generation for grounded answers\n",
    "\n",
    "**Advanced Integration:**\n",
    "- Combining function tools with RAG\n",
    "- Building complete research assistants that can both calculate and retrieve information\n",
    "- Understanding vector databases for production systems\n",
    "\n",
    "**What You Can Build Now:**\n",
    "\n",
    "With these skills, you can create:\n",
    "1. **Custom research assistants** that answer questions about your papers using RAG\n",
    "2. **Data analysis tools** that combine calculations with natural language reasoning\n",
    "3. **Observatory interfaces** that let you query instruments and databases conversationally\n",
    "4. **Automated workflows** that process data and generate reports\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "**Practice Projects:**\n",
    "1. Build a RAG system for your research group's papers\n",
    "2. Create function tools for your most common calculations\n",
    "3. Combine both to build a complete research assistant\n",
    "\n",
    "**Production Considerations:**\n",
    "- Implement vector databases (Pinecone, Weaviate, ChromaDB) for large-scale RAG\n",
    "- Add proper error handling and logging\n",
    "- Set up monitoring for costs and performance\n",
    "- Consider fine-tuning for domain-specific tasks\n",
    "\n",
    "**Scaling Up:**\n",
    "- For large document collections (1000+ papers), use vector databases\n",
    "- For complex workflows, explore LangChain or LlamaIndex frameworks\n",
    "- For production systems, implement caching and rate limiting\n",
    "\n",
    "**Resources:**\n",
    "- Anthropic Tool Use Guide: https://docs.anthropic.com/claude/docs/tool-use\n",
    "- Sentence Transformers: https://www.sbert.net/\n",
    "- Vector Databases comparison: https://www.pinecone.io/learn/vector-database/\n",
    "- This tutorial's GitHub: https://github.com/tingyuansen/NASA_AI_ML_STIG\n",
    "\n",
    "**The Future:**\n",
    "\n",
    "LLMs are evolving rapidly. The patterns you've learned—APIs, function tools, RAG—form the foundation \n",
    "that will remain relevant even as models improve. You're now equipped to integrate AI into your \n",
    "research workflow and adapt as new capabilities emerge.\n",
    "\n",
    "Thank you for working through these tutorials. Now go build something amazing! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
