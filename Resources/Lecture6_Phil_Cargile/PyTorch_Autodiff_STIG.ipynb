{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfefdb0",
   "metadata": {},
   "source": [
    "# <u> Automatic Differentiation in PyTorch </u>\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Review basic PyTorch tensors\n",
    "2. Introduce automatic differentiation (`autograd`)\n",
    "3. Revisit our toy example from the slides\n",
    "4. Use autodiff for optimization\n",
    "\n",
    "The goal is **intuition**, not speed or deep learning tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa469ae",
   "metadata": {},
   "source": [
    "## PyTorch tensors (the basics)\n",
    "\n",
    "A PyTorch `Tensor` is similar to a NumPy array:\n",
    "- N-dimensional\n",
    "- Supports vectorized operations\n",
    "- Can live on CPU or GPU (if CUDA is available)\n",
    "\n",
    "Unlike NumPy arrays, PyTorch tensors can also:\n",
    "- **Track operations (Needed for AD)**\n",
    "- **Compute derivatives automatically**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # import the PyTorch library\n",
    "import numpy as np # import the NumPy library\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0]) # create a 1D tensor \n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = np.array([4.0, 5.0, 6.0]) # create a NumPy array\n",
    "print(y)\n",
    "print(type(y))\n",
    "\n",
    "x = torch.from_numpy(y) # convert NumPy array to PyTorch tensor\n",
    "print(x)\n",
    "print(type(x))\n",
    "\n",
    "y = x.numpy() # convert PyTorch tensor back to NumPy array\n",
    "print(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a1aac",
   "metadata": {},
   "source": [
    "PyTorch treats **scalars** as 0-dimensional tensors and **vectors** as 1-D tensors.\n",
    "\n",
    "This matters for autodiff:\n",
    "- Gradients are defined for **scalars**\n",
    "- Most ML losses are scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4d6ee",
   "metadata": {},
   "source": [
    "### Tracking derivatives with `requires_grad`\n",
    "\n",
    "To tell PyTorch that we want derivatives with respect to a tensor,\n",
    "we set: `requires_grad=True` when creating it.\n",
    "\n",
    "From this point on, PyTorch:\n",
    "- Builds a **computational graph**\n",
    "- Stores how each operation depends on this tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259c13df",
   "metadata": {},
   "source": [
    "### Toy example\n",
    "\n",
    "From the slides, we consider the function:\n",
    "\n",
    "y = sin(x) * x^2\n",
    "\n",
    "We will:\n",
    "1. Evaluate this function\n",
    "2. Compute its derivative using autodiff\n",
    "3. Compare against the analytic result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our function y = sin(x) * x^2\n",
    "# Notice the use of torch sine function\n",
    "def f(x): \n",
    "    return torch.sin(x) * x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae4057",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "First, we simply *evaluate* the function.\n",
    "\n",
    "This is just normal numerical computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.234, requires_grad=True)\n",
    "y = f(x)\n",
    "print(f\"y = {y.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d51552f",
   "metadata": {},
   "source": [
    "### Backward pass (reverse-mode autodiff)\n",
    "\n",
    "Calling `.backward()`:\n",
    "- Starts from the output `y`\n",
    "- Walks backward through the computational graph\n",
    "- Applies the chain rule automatically\n",
    "\n",
    "This computes:\n",
    "dy/dx and stores it in `x.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # do the backward pass through f(x)\n",
    "print(\"dy/dx =\", x.grad.item()) # print the gradient, the .item() gets the raw value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276d055",
   "metadata": {},
   "source": [
    "### Consistency check: analytic derivative\n",
    "\n",
    "Analytically:\n",
    "\n",
    "df/dx = d[sin(x)x^2]/dx = 2x * sin(x) + x^2 * cos(x)\n",
    "\n",
    "Autodiff should match this exactly (up to floating-point precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f72a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x.detach().numpy() # must detach the PyTorch tensor before converting to a NumPy array if it is tracking gradients\n",
    "analytic = 2*x_val * np.sin(x_val) + (x_val**2) * np.cos(x_val)\n",
    "\n",
    "print(f\"autodiff : {x.grad}\")\n",
    "print(f\"analytic : {analytic}\")\n",
    "print(f\"difference: {abs(x.grad.numpy() - analytic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.234, requires_grad=True) # redefine x and its gradient\n",
    "\n",
    "f(x).backward() # first backward pass (computes dy/dx)\n",
    "print(\"after first backward:\", x.grad)\n",
    "\n",
    "f(x).backward() # second backward pass (computes dy/dx again and adds to existing gradient)\n",
    "print(\"after second backward:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8681ccc6",
   "metadata": {},
   "source": [
    "### Clearing gradients\n",
    "\n",
    "In optimization loops, we usually clear gradients explicitly using:\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_() # clear existing gradient\n",
    "f(x).backward() # compute gradient again\n",
    "print(\"after zero_ then backward:\", x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746d83e",
   "metadata": {},
   "source": [
    "### Optimization using autodiff\n",
    "\n",
    "Now we use gradients to *optimize* a parameter.\n",
    "\n",
    "This is exactly what training a neural network does —\n",
    "just with many more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = torch.tensor(1.0) # the target value for y, i.e., we want to find x such that f(x) = 1.0\n",
    "\n",
    "x = torch.tensor(2.5, requires_grad=True) # initial guess for x\n",
    "lr = 1E-3 # learning rate, this is a hyperparameter that defines the step size in each iteration\n",
    "\n",
    "# optimization loop, lets run for 100 steps\n",
    "for step in range(100):\n",
    "\n",
    "    # evaluate function and compute loss\n",
    "    y = f(x) # compute current value of y\n",
    "    loss = (y - y_target)**2 # mean squared error loss\n",
    "    loss.backward() # compute gradient dy/dx\n",
    "\n",
    "    # the optimizer is not part of the computational graph, so we use torch.no_grad() \n",
    "    # on the update step. This prevents PyTorch from tracking operations on x during the update.\n",
    "    with torch.no_grad(): # disable gradient tracking for the update step\n",
    "        x -= lr * x.grad # gradient descent update, move x in the direction of negative gradient\n",
    "        x.grad.zero_() # clear gradients for the next iteration\n",
    "\n",
    "    # print progress every 5 steps\n",
    "    if step % 5 == 0 or step == 99:\n",
    "        print(f\"step {step:02d}  x={x.item(): .4f}  y={y.item(): .4f}  log(loss)={np.log10(loss.item()): .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0647451",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "PyTorch Automatic differentiation:\n",
    "- Tracks computation through backward pass\n",
    "- Applies the chain rule automatically\n",
    "- Scales to millions of parameters\n",
    "\n",
    "This is the foundation of modern scientific ML and deep learning with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd266b",
   "metadata": {},
   "source": [
    "## `torch.nn` module\n",
    "In practice, we usually define models as subclasses of `torch.nn.Module`. This container is just a Python object that knows how to compute a forward pass and holds trainable parameters (parameters that we want to fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f28644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the neural network module from PyTorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define a simple model as a subclass of nn.Module\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = nn.Parameter(torch.tensor(2.5)) # define a trainable parameter x\n",
    "\n",
    "    # the forward method defines the computation performed at every call\n",
    "    def forward(self):\n",
    "        return torch.sin(self.x) * self.x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model object\n",
    "toymodel = ToyModel()\n",
    "\n",
    "# perform a forward pass to compute the output\n",
    "y = toymodel()\n",
    "print(f\"y = {y}\")\n",
    "\n",
    "print(f\"model parameter x = {toymodel.x.item()}\") # access the model parameter x, using item to get the Python number\n",
    "\n",
    "# lets see the model parameters\n",
    "for name, p in toymodel.named_parameters():\n",
    "    print(f\"parameter name: {name}, value: {p.item()}, requires_grad: {p.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288b2f0",
   "metadata": {},
   "source": [
    "## `torch.optim` module\n",
    "PyTorch provides an `optim` module that implements various optimization algorithms (e.g., SGD, Adam). This module simplifies the optimization process by handling parameter updates based on computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim # import the optim module from PyTorch\n",
    "y_target = torch.tensor(1.0) # target value for y, we want to find x such that model(x) = 1.0\n",
    "\n",
    "# use the SGD optimizer from torch.optim instead of our manual gradient descent\n",
    "toyoptimizer = torch.optim.SGD(toymodel.parameters(), lr=1E-3) # create an SGD optimizer to update model parameters\n",
    "\n",
    "# optimization loop using the optimizer\n",
    "for step in range(100):\n",
    "    \n",
    "    # forward pass and compute loss\n",
    "    y = toymodel()\n",
    "    loss = (y - y_target)**2\n",
    "\n",
    "    # backward pass to compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters using the optimizer\n",
    "    toyoptimizer.step()\n",
    "\n",
    "    # clear existing gradients\n",
    "    toyoptimizer.zero_grad()\n",
    "\n",
    "    # print progress every 5 steps\n",
    "    if step % 5 == 0 or step == 99:\n",
    "        print(f\"step {step:02d}  x={toymodel.x.item(): .4f}  y={y.item(): .4f}  log(loss)={np.log10(loss.item()): .4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea4409",
   "metadata": {},
   "source": [
    "# <u> CPU vs GPU in PyTorch </u>\n",
    "\n",
    "PyTorch tensors live on a **device**:\n",
    "- CPU (default)\n",
    "- GPU (CUDA), if available (Metal for Mac is in beta, not covered here but might be available and useful for Apple Silicon Macs)\n",
    "\n",
    "Operations happen on the device where the tensors live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552dd588",
   "metadata": {},
   "source": [
    "## Moving tensors between devices\n",
    "\n",
    "We move tensors explicitly using `.to(device)`.\n",
    "\n",
    "All tensors involved in a computation must live on the **same device**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad444292",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.tensor(1.234, requires_grad=True)\n",
    "print(\"x device:\", x.device)\n",
    "\n",
    "x = x.to(device)\n",
    "print(\"x device after move:\", x.device)\n",
    "\n",
    "y = torch.sin(x) * x**2\n",
    "y.backward()\n",
    "\n",
    "print(\"dy/dx:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ab178",
   "metadata": {},
   "source": [
    "## Models and devices\n",
    "\n",
    "Models are just collections of tensors.\n",
    "\n",
    "Moving a model to a GPU moves *all of its parameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's move a model to the GPU if available and perform a forward pass\n",
    "\n",
    "toymodel = ToyModel() # initialize the model object\n",
    "toymodel = toymodel.to(device) # move the model to the selected device\n",
    "\n",
    "# print the model parameters and their devices\n",
    "for name, p in toymodel.named_parameters():\n",
    "    print(f\"parameter name: {name}, value: {p.item()}, requires_grad: {p.requires_grad}, device: {p.device} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818cc97",
   "metadata": {},
   "source": [
    "Everything else behaves the same way as on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a forward pass on the selected device\n",
    "target = torch.tensor(1.0).to(device) # move target to the selected device\n",
    "toyoptimizer = torch.optim.SGD(toymodel.parameters(), lr=1E-3) # create an SGD optimizer to update model parameters, nothing different here \n",
    "\n",
    "# Doing one optimization step\n",
    "toyoptimizer.zero_grad()\n",
    "y = toymodel()\n",
    "loss = (y - target)**2\n",
    "loss.backward()\n",
    "toyoptimizer.step()\n",
    "\n",
    "print(\"x =\", toymodel.x.item(), \"loss =\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4734df",
   "metadata": {},
   "source": [
    "# <u> Minimal MLP demo: fitting a parabola </u>\n",
    "\n",
    "We’ll train a tiny neural network to learn a function from data.\n",
    "\n",
    "**Task:** learn \\( y = x^2 \\) from sampled points.\n",
    "\n",
    "This will highlight:\n",
    "- building a model (`nn.Module`)\n",
    "- training loop with `torch.optim`\n",
    "- a minimal `Dataset` + `DataLoader`\n",
    "- (optional) GPU offloading via `.to(device)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695a283",
   "metadata": {},
   "source": [
    "## Synthetic dataset\n",
    "\n",
    "We sample \"x\" uniformly and generate:\n",
    "\n",
    "y = x^2 + ε\n",
    "\n",
    "where \"ε\" is small Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random number seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# sample synthetic dataset from the function y = x^2 + noise\n",
    "N = 2048 # number of data points\n",
    "x = 2.0 * torch.rand(N, 1) - 1.0          # x in [-1, 1], shape (N, 1)\n",
    "y = x**2 + 0.03 * torch.randn(N, 1)        # noisy parabola, shape (N, 1)\n",
    "\n",
    "# Train/val split\n",
    "n_train = int(0.8 * N) # number of training data points, 80% for training and 20% for validation\n",
    "x_train, x_valid = x[:n_train], x[n_train:]\n",
    "y_train, y_valid = y[:n_train], y[n_train:]\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train) # create training dataset\n",
    "val_ds   = TensorDataset(x_valid, y_valid) # create validation dataset\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True) # create training data loader\n",
    "valid_loader   = DataLoader(val_ds, batch_size=256, shuffle=False) # create validation data loader\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"val batches:\", len(valid_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6400ebc",
   "metadata": {},
   "source": [
    "## Model: a tiny MLP\n",
    "\n",
    "We’ll use a fully-connected network:\n",
    "\n",
    "- input: 1 number (x)\n",
    "- hidden: 32 → 32\n",
    "- output: 1 number (y)\n",
    "\n",
    "Activation: **ReLU** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: a tiny MLP\n",
    "class ParabolaMLP(nn.Module):\n",
    "    def __init__(self, hidden=32):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden # store hidden size\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, self.hidden), # input layer size: (x) 1 -> hidden size\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.Linear(self.hidden, self.hidden), # hidden layer: hidden size -> hidden size\n",
    "            nn.ReLU(), # activation function\n",
    "            nn.Linear(self.hidden, 1), # output layer size: hidden size -> 1 (y)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "mlp = ParabolaMLP(hidden=32).to(device)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7e939",
   "metadata": {},
   "source": [
    "## Training setup\n",
    "\n",
    "Loss function: Minimize mean squared error (MSE)\n",
    "\n",
    "Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss() # loss function\n",
    "mlpopt = optim.Adam(mlp.parameters(), lr=1E-3) # optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423391b4",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Each epoch:\n",
    "1. loop over mini-batches from the DataLoader\n",
    "2. forward pass → loss\n",
    "3. `loss.backward()` computes gradients\n",
    "4. `opt.step()` updates parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b490b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make code modular by defining an evaluation function\n",
    "def evaluate(model, loader):\n",
    "    \n",
    "    model.eval() # change model to evaluation mode, disabling dropout/batchnorm if any\n",
    "    total_loss = 0.0 # initialize total loss\n",
    "    n = 0 # initialize number of samples\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # iterate over batches in the data loader\n",
    "        for xb, yb in loader:\n",
    "\n",
    "            xb = xb.to(device) # move input batch to device\n",
    "            yb = yb.to(device) # move target batch to device\n",
    "\n",
    "            pred = model(xb) # make model prediction\n",
    "            loss = loss_fn(pred, yb) # compute loss\n",
    "            \n",
    "            total_loss += loss.item() * xb.size(0) # accumulate total loss over all samples\n",
    "            n += xb.size(0) # update number of samples\n",
    "            \n",
    "    return total_loss / n # return average loss over all samples\n",
    "\n",
    "# define number of training epochs\n",
    "epochs = 100\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    mlp.train() # change model to training mode, turns on dropout/batchnorm if any\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device) # move input batch to device\n",
    "        yb = yb.to(device) # move target batch to device\n",
    "\n",
    "        ypred = mlp(xb) # forward pass\n",
    "        loss = loss_fn(ypred, yb) # compute loss\n",
    "\n",
    "        loss.backward() # backward pass to compute gradients\n",
    "\n",
    "        mlpopt.step() # update model parameters in the optimizer\n",
    "\n",
    "        mlpopt.zero_grad() # clear existing gradients\n",
    "\n",
    "    # evaluate and print losses every 10 epochs\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        train_loss = evaluate(mlp, train_loader) # evaluate on training set\n",
    "        valid_loss = evaluate(mlp, valid_loader) # evaluate on validation set\n",
    "        print(f\"epoch {epoch:03d} | train log(MSE)={np.log10(train_loss):.5f} | val log(MSE)={np.log10(valid_loss):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46794d",
   "metadata": {},
   "source": [
    "## Visualizing the learned function\n",
    "\n",
    "We compare:\n",
    "- the noisy training data\n",
    "- the true function \\(y = x^2\\)\n",
    "- the neural network prediction\n",
    "\n",
    "This helps us see what the MLP actually learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cce209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # import matplotlib for plotting\n",
    "\n",
    "# Put model in eval mode\n",
    "mlp.eval()\n",
    "\n",
    "# Generate a smooth x grid\n",
    "x_plot = torch.linspace(-1.0, 1.0, 400).unsqueeze(1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = mlp(x_plot).cpu()\n",
    "\n",
    "# Move to CPU for plotting\n",
    "x_plot = x_plot.cpu()\n",
    "y_true = x_plot**2\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6, 4),layout='constrained')\n",
    "\n",
    "# Training data (subsample for clarity)\n",
    "ax.scatter(\n",
    "    x_train[::10].numpy(),\n",
    "    y_train[::10].numpy(),\n",
    "    s=10,\n",
    "    alpha=0.4,\n",
    "    label=\"training data\",\n",
    ")\n",
    "\n",
    "# True function\n",
    "ax.plot(\n",
    "    x_plot.numpy(),\n",
    "    y_true.numpy(),\n",
    "    linewidth=2,\n",
    "    label=\"true y = x²\",\n",
    ")\n",
    "\n",
    "# Model prediction\n",
    "ax.plot(\n",
    "    x_plot.numpy(),\n",
    "    y_pred.numpy(),\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\",\n",
    "    label=\"MLP prediction\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91b34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
